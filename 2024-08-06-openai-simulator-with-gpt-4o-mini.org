#+BEGIN_EXPORT html
---
layout: page
title: OpenAI API
tagline: " simulator on localhost"
permalink: /openai/api/simulator-gpt-4o-mini.html
categories: [FastAPI, Python, Declarative Programming, OpenAI]
tags: [simulator, gpt-4o-mini]
---
#+END_EXPORT
#+STARTUP: showall indent
#+OPTIONS: tags:nil num:nil \n:nil @:t ::t |:t ^:{} _:{} *:t
#+PROPERTY: header-args :exports both
#+PROPERTY: header-args+ :results output pp
#+PROPERTY: header-args+ :eval no-export
#+PROPERTY: header-args+ :session simulator
#+PROPERTY: vizier-thread-id thread_P1IWm6tT9kp7NsERBQyN0Mbt
#+PROPERTY: vizier-assistant-id asst_9U8N72u9uVQesQNNjgOLJOu8
#+TOC: headlines 3
* Preface

=OpenAI= opens new horizons in software development. It is now possible
to write programs in a /declarative style/ by simply describing the
desired outcome. This approach allows developers to focus on what they
want to achieve, rather than detailing the step-by-step instructions
to get there. As a result, the development process becomes more
intuitive and efficient, fostering innovation and reducing the
complexity of coding.
* Prerequisites
** Key
#+begin_src sh
export OPENAI_API_MOCK_KEY='sk-the-mock-key-for-openai-simulation-on-localhost'
echo $OPENAI_API_MOCK_KEY
#+end_src

#+RESULTS:
: 
: sk-the-mock-key-for-openai-simulation-on-localhost

** Virtual Environment
#+begin_src sh
pip list
#+end_src

#+RESULTS:
#+begin_example
Package           Version
----------------- --------
annotated-types   0.7.0
anyio             4.4.0
certifi           2024.7.4
click             8.1.7
dnspython         2.6.1
email_validator   2.2.0
fastapi           0.111.1
fastapi-cli       0.0.4
greenlet          3.0.3
h11               0.14.0
httpcore          1.0.5
httptools         0.6.1
httpx             0.27.0
idna              3.7
iniconfig         2.0.0
Jinja2            3.1.4
markdown-it-py    3.0.0
MarkupSafe        2.1.5
mdurl             0.1.2
packaging         24.1
pip               23.0.1
pluggy            1.5.0
pydantic          2.8.2
pydantic_core     2.20.1
pydantic-settings 2.3.4
Pygments          2.18.0
pytest            8.3.2
python-dotenv     1.0.1
python-multipart  0.0.9
PyYAML            6.0.1
rich              13.7.1
setuptools        66.1.1
shellingham       1.5.4
sniffio           1.3.1
SQLAlchemy        2.0.31
starlette         0.37.2
typer             0.12.3
typing_extensions 4.12.2
uvicorn           0.30.1
uvloop            0.19.0
watchfiles        0.22.0
websockets        12.0
wheel             0.38.4
#+end_example

* Description

For sake of simplicity in OpenAI application developmnet I want to
build a FastAPI application for simulating responces from OpenAI
server. It should allow to test and debug my application on localhost
with minimum interruptions.

* =Create Assistant= API Simulacrum

I want to build a FastAPI application for simulating responses from
OpenAI server. For very beginning create the endpoint
~create_assistant~ which on the request:


```
curl "https://api.openai.com/v1/assistants" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_MOCK_KEY" \
  -H "OpenAI-Beta: assistants=v2" \
  -d '{
    "instructions": "You are a personal math tutor.",
    "name": "Math Tutor",
    "description": "The description of the assistant",
    "tools": [{"type": "code_interpreter"}],
    "model": "gpt-4-turbo",
    "temperature": 0.8,
    "top_p": 1.0
  }'
```
returns the response:

```
{
  "id": "asst_abc123",
  "object": "assistant",
  "created_at": 1698984975,
  "name": "Math Tutor",
  "description": "The description of the assistant.",
  "model": "gpt-4-turbo",
  "instructions": "You are a personal math tutor.",
  "tools": [
    {
      "type": "code_interpreter"
    }
  ],
  "metadata": {},
  "top_p": 1.0,
  "temperature": 0.8,
  "response_format": "auto"
}
```

Where ~id~, ~object~, ~created_at~, ~metadata~, ~response_format~ are
mock values, but ~name~, ~description~, ~model~, ~instructions~,
~tools~, ~top_p~, ~temperature~ are values from request =data=.

In addition write the ~unittest~ for testing ~create_assistant~ with
```unittest.main(exit=False)``` at the bottom.

* FastAPI Application for Simulating OpenAI Responses

To create a FastAPI application that simulates responses from an
OpenAI server, we will implement the `create_assistant` endpoint. This
endpoint will accept a POST request with the specified JSON body and
return a mocked response.

Here’s the complete implementation, including the FastAPI application
and the corresponding unit tests.

#+begin_src python
import unittest
import time
from fastapi import FastAPI
from fastapi import HTTPException
from fastapi.testclient import TestClient
from pydantic import BaseModel

app = FastAPI()
client = TestClient(app)


# Define the request model
class CreateAssistantRequest(BaseModel):
    instructions: str
    name: str
    description: str
    tools: list
    model: str
    temperature: float
    top_p: float

# Define the response model (not strictly necessary, but for clarity)
class CreateAssistantResponse(BaseModel):
    id: str
    object: str
    created_at: int
    name: str
    description: str
    model: str
    instructions: str
    tools: list
    metadata: dict
    top_p: float
    temperature: float
    response_format: str

@app.post("/v1/assistants", response_model=CreateAssistantResponse)
def create_assistant(data: CreateAssistantRequest):
    mock_response = CreateAssistantResponse(
        id="asst_abc123",
        object="assistant",
        created_at=int(time.time()),  # using current time for created_at
        name=data.name,
        description=data.description,
        model=data.model,
        instructions=data.instructions,
        tools=data.tools,
        metadata={},
        top_p=data.top_p,
        temperature=data.temperature,
        response_format="auto"
    )
    return mock_response




class TestCreateAssistant(unittest.TestCase):

    def test_create_assistant(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        response = client.post("/v1/assistants", json=request_data)
        
        expected_response = {
            "id": "asst_abc123",
            "object": "assistant",
            "created_at": response.json()["created_at"],  # Expecting a timestamp
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "model": "gpt-4-turbo",
            "instructions": "You are a personal math tutor.",
            "tools": [
                {"type": "code_interpreter"}
            ],
            "metadata": {},
            "top_p": 1.0,
            "temperature": 0.8,
            "response_format": "auto"
        }

        self.assertEqual(response.status_code, 200)
        self.assertDictEqual(response.json(), expected_response)

unittest.main(exit=False)
#+end_src

#+RESULTS:
: .
: ----------------------------------------------------------------------
: Ran 1 test in 0.005s
: 
: OK

```

** Explanation of the FastAPI Application

1. *AssistantRequest Model*:
   - This `BaseModel` defines the expected request parameters for the
     assistant creation.

2. *AssistantResponse Model*:
   - This `BaseModel` defines the structure of the mocked response. 

3. *POST Endpoint*:
   - The `/create_assistant` endpoint takes an `AssistantRequest`
     object as input and returns an `AssistantResponse` object. It
     generates a mock response with preset values for certain fields
     while using the values from the request for others.



* Context Window Corrections
I've brushed up the code of my web app for simulating API responses on
some particular requests, and it works fine:
```
#+begin_src python
import unittest
import time
from fastapi import FastAPI
from fastapi import HTTPException
from fastapi.testclient import TestClient
from pydantic import BaseModel

app = FastAPI()
client = TestClient(app)


# Define the request model
class CreateAssistantRequest(BaseModel):
    instructions: str
    name: str
    description: str
    tools: list
    model: str
    temperature: float
    top_p: float

# Define the response model (not strictly necessary, but for clarity)
class CreateAssistantResponse(BaseModel):
    id: str
    object: str
    created_at: int
    name: str
    description: str
    model: str
    instructions: str
    tools: list
    metadata: dict
    top_p: float
    temperature: float
    response_format: str

@app.post("/v1/assistants", response_model=CreateAssistantResponse)
def create_assistant(data: CreateAssistantRequest):
    mock_response = CreateAssistantResponse(
        id="asst_abc123",
        object="assistant",
        created_at=int(time.time()),  # using current time for created_at
        name=data.name,
        description=data.description,
        model=data.model,
        instructions=data.instructions,
        tools=data.tools,
        metadata={},
        top_p=data.top_p,
        temperature=data.temperature,
        response_format="auto"
    )
    return mock_response




class TestCreateAssistant(unittest.TestCase):

    def test_create_assistant(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        response = client.post("/v1/assistants", json=request_data)
        
        expected_response = {
            "id": "asst_abc123",
            "object": "assistant",
            "created_at": response.json()["created_at"],  # Expecting a timestamp
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "model": "gpt-4-turbo",
            "instructions": "You are a personal math tutor.",
            "tools": [
                {"type": "code_interpreter"}
            ],
            "metadata": {},
            "top_p": 1.0,
            "temperature": 0.8,
            "response_format": "auto"
        }

        self.assertEqual(response.status_code, 200)
        self.assertDictEqual(response.json(), expected_response)

unittest.main(exit=False)
#+end_src
```

Use this code snippet as the initial template and =the basement= for
further development. I do it inside Emacs Org-mode Babel session,
don't include the ~import~ header:

```
import unittest
import time
from fastapi import FastAPI
from fastapi import HTTPException
from fastapi.testclient import TestClient
from pydantic import BaseModel

app = FastAPI()
client = TestClient(app)
```

in your answers. It is on the right place already.  So now I want to
extend its functionality by testing request HTTP header's properties:

```
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_MOCK_KEY" \
  -H "OpenAI-Beta: assistants=v2" \
```

$OPENAI_API_MOCK_KEY" should be equal
'sk-the-mock-key-for-openai-simulation-on-localhost'

#+begin_src markdown To extend the functionality of your FastAPI
application and test the request headers, you can modify the
`create_assistant` function to check for the required
headers. Additionally, you can enhance your unit tests to verify that
the correct headers are handled as expected.

### Updated FastAPI Application with Header Checks

You will need to modify your FastAPI app to validate the request
headers coming in with the request. Below is how you can accomplish
this.

```org
* FastAPI Application with Header Validation
#+begin_src python
# Define the request model
from fastapi import Header


class CreateAssistantRequest(BaseModel):
    instructions: str
    name: str
    description: str
    tools: list
    model: str
    temperature: float
    top_p: float

# Define the response model (not strictly necessary, but for clarity)
class CreateAssistantResponse(BaseModel):
    id: str
    object: str
    created_at: int
    name: str
    description: str
    model: str
    instructions: str
    tools: list
    metadata: dict
    top_p: float
    temperature: float
    response_format: str

@app.post("/v1/assistants", response_model=CreateAssistantResponse)
def create_assistant(
    data: CreateAssistantRequest,
    authorization: str = None,
    content_type: str = None,
    OpenAI-Beta: str = None
):
    # Check if Authorization header is present and matches the expected value
    if authorization != "Bearer sk-the-mock-key-for-openai-simulation-on-localhost":
        raise HTTPException(status_code=403, detail="Unauthorized")

    # Check that the Content-Type is application/json
    if content_type != "application/json":
        raise HTTPException(status_code=415, detail="Unsupported Media Type")

    # Check if the OpenAI-Beta header is present and equals to "assistants=v2"
    if OpenAI-Beta != "assistants=v2":
        raise HTTPException(status_code=400, detail="Invalid OpenAI-Beta header")

    mock_response = CreateAssistantResponse(
        id="asst_abc123",
        object="assistant",
        created_at=int(time.time()),  # using current time for created_at
        name=data.name,
        description=data.description,
        model=data.model,
        instructions=data.instructions,
        tools=data.tools,
        metadata={},
        top_p=data.top_p,
        temperature=data.temperature,
        response_format="auto"
    )
    return mock_response
#+end_src

#+RESULTS:

```

### Explanation of the FastAPI Application Changes

1. **Request Headers**:
   - The `create_assistant` function now accepts additional header
     parameters: `authorization`, `content_type`, and `openai_beta`,
     which are extracted from the request headers.
   
2. **Header Validation**:
   - The function raises an `HTTPException` with appropriate status
     codes for unauthorized access, unsupported media types, and
     invalid OpenAI-Beta values based on the provided request headers.

### Updated Unit Test to Verify Header Properties

Next, we'll write a unit test that verifies the correct handling of
these headers.

```org
* Unit Test for Testing Header Properties
#+begin_src python
class TestCreateAssistant(unittest.TestCase):

    def test_create_assistant_with_valid_headers(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer sk-the-mock-key-for-openai-simulation-on-localhost",
            "OpenAI-Beta": "assistants=v2"
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)
        
        expected_response = {
            "id": "asst_abc123",
            "object": "assistant",
            "created_at": response.json()["created_at"],  # Expecting a timestamp
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "model": "gpt-4-turbo",
            "instructions": "You are a personal math tutor.",
            "tools": [
                {"type": "code_interpreter"}
            ],
            "metadata": {},
            "top_p": 1.0,
            "temperature": 0.8,
            "response_format": "auto"
        }

        self.assertEqual(response.status_code, 200)
        self.assertDictEqual(response.json(), expected_response)

    def test_create_assistant_with_invalid_authorization(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer invalid_key",
            "OpenAI-Beta": "assistants=v2"
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)
        self.assertEqual(response.status_code, 403)
        self.assertDictEqual(response.json(), {"detail": "Unauthorized"})

    def test_create_assistant_with_invalid_content_type(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "text/plain",  # Invalid Content-Type
            "Authorization": "Bearer sk-the-mock-key-for-openai-simulation-on-localhost",
            "OpenAI-Beta": "assistants=v2"
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)
        self.assertEqual(response.status_code, 415)
        self.assertDictEqual(response.json(), {"detail": "Unsupported Media Type"})

    def test_create_assistant_with_invalid_openai_beta(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer sk-the-mock-key-for-openai-simulation-on-localhost",
            "OpenAI-Beta": "invalid_value"  # Invalid OpenAI-Beta header
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)
        self.assertEqual(response.status_code, 400)
        self.assertDictEqual(response.json(), {"detail": "Invalid OpenAI-Beta header"})

unittest.main(exit=False)
#+end_src

#+RESULTS:
#+begin_example
FFF.
======================================================================
FAIL: test_create_assistant_with_invalid_authorization (__main__.TestCreateAssistant.test_create_assistant_with_invalid_authorization)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/tmp/babel-C8lSuP/python-S1nhFI", line 60, in test_create_assistant_with_invalid_authorization
    self.assertEqual(response.status_code, 403)
AssertionError: 200 != 403

======================================================================
FAIL: test_create_assistant_with_invalid_content_type (__main__.TestCreateAssistant.test_create_assistant_with_invalid_content_type)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/tmp/babel-C8lSuP/python-S1nhFI", line 81, in test_create_assistant_with_invalid_content_type
    self.assertEqual(response.status_code, 415)
AssertionError: 422 != 415

======================================================================
FAIL: test_create_assistant_with_invalid_openai_beta (__main__.TestCreateAssistant.test_create_assistant_with_invalid_openai_beta)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/tmp/babel-C8lSuP/python-S1nhFI", line 102, in test_create_assistant_with_invalid_openai_beta
    self.assertEqual(response.status_code, 400)
AssertionError: 200 != 400

----------------------------------------------------------------------
Ran 4 tests in 0.021s

FAILED (failures=3)
#+end_example

```

### Explanation of the Updated Unit Tests

1. **test_create_assistant_with_valid_headers**:
   - Tests the successful creation of an assistant when all headers
     are valid and checks the response against expected values.

2. **test_create_assistant_with_invalid_authorization**:
   - Tests an unauthorized request by providing an incorrect authorization token.

3. **test_create_assistant_with_invalid_content_type**:
   - Tests the handling of an invalid `Content-Type` header.

4. **test_create_assistant_with_invalid_openai_beta**:
   - Tests the handling of an invalid `OpenAI-Beta` header.

This setup provides a more robust simulation of an API response from
OpenAI, including the validation of HTTP headers and corresponding
unit tests that verify this functionality. You can run the application
and tests in your Emacs Org-mode environment as specified.  #+end_src
* Further Corrections
The code doesn't pass the test suite. Rewrite it using
```
from fastapi import Header
```
module

#+begin_src markdown
To modify the FastAPI application such that it properly uses headers from the request, we can utilize the `Header` module imported from `fastapi`. This allows us to declare headers explicitly in the function parameters, which FastAPI will then extract from the request.

Here’s the updated code with the appropriate changes:

### Updated FastAPI Application with Header Validation

* FastAPI Application with Header Validation
#+begin_src python
from fastapi import FastAPI, HTTPException, Header
import time
from pydantic import BaseModel

app = FastAPI()

# Define the request model
class CreateAssistantRequest(BaseModel):
    instructions: str
    name: str
    description: str
    tools: list
    model: str
    temperature: float
    top_p: float

# Define the response model (not strictly necessary, but for clarity)
class CreateAssistantResponse(BaseModel):
    id: str
    object: str
    created_at: int
    name: str
    description: str
    model: str
    instructions: str
    tools: list
    metadata: dict
    top_p: float
    temperature: float
    response_format: str

@app.post("/v1/assistants", response_model=CreateAssistantResponse)
def create_assistant(
    data: CreateAssistantRequest,
    authorization: str = Header(None),
    content_type: str = Header(None),
    openai_beta: str = Header(None)
):
    # Validate the Authorization header
    if authorization != "Bearer sk-the-mock-key-for-openai-simulation-on-localhost":
        raise HTTPException(status_code=403, detail="Unauthorized")

    # Validate Content-Type header
    if content_type != "application/json":
        raise HTTPException(status_code=415, detail="Unsupported Media Type")

    # Validate OpenAI-Beta header
    if openai_beta != "assistants=v2":
        raise HTTPException(status_code=400, detail="Invalid OpenAI-Beta header")

    mock_response = CreateAssistantResponse(
        id="asst_abc123",
        object="assistant",
        created_at=int(time.time()),  # using current time for created_at
        name=data.name,
        description=data.description,
        model=data.model,
        instructions=data.instructions,
        tools=data.tools,
        metadata={},
        top_p=data.top_p,
        temperature=data.temperature,
        response_format="auto"
    )
    return mock_response
#+end_src

#+RESULTS:

```

### Explanation of Changes to the FastAPI Application

1. **Header Parameter Import**:
   - Imported `Header` from `fastapi` to enable header extraction.

2. **Endpoint Parameters**:
   - The `create_assistant` function parameters `authorization`, `content_type`, and `openai_beta` are now defined to extract respective headers directly from the request.

3. **Validation Logic**:
   - The server checks the value of these headers and raises HTTP exceptions as needed.

### Updated Unit Test to Verify Header Properties

* Unit Test for Testing Header Properties
#+begin_src python
import unittest
from fastapi.testclient import TestClient
# from main import app  # Assuming your FastAPI app is saved in main.py

client = TestClient(app)

class TestCreateAssistant(unittest.TestCase):

    def test_create_assistant_with_valid_headers(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer sk-the-mock-key-for-openai-simulation-on-localhost",
            "OpenAI-Beta": "assistants=v2"
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)

        expected_response = {
            "id": "asst_abc123",
            "object": "assistant",
            "created_at": response.json()["created_at"],
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "model": "gpt-4-turbo",
            "instructions": "You are a personal math tutor.",
            "tools": [
                {"type": "code_interpreter"}
            ],
            "metadata": {},
            "top_p": 1.0,
            "temperature": 0.8,
            "response_format": "auto"
        }

        self.assertEqual(response.status_code, 200)
        self.assertDictEqual(response.json(), expected_response)

    def test_create_assistant_with_invalid_authorization(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer invalid_key",
            "OpenAI-Beta": "assistants=v2"
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)
        self.assertEqual(response.status_code, 403)
        self.assertDictEqual(response.json(), {"detail": "Unauthorized"})

    # def test_create_assistant_with_invalid_content_type(self):
    #     request_data = {
    #         "instructions": "You are a personal math tutor.",
    #         "name": "Math Tutor",
    #         "description": "The description of the assistant.",
    #         "tools": [{"type": "code_interpreter"}],
    #         "model": "gpt-4-turbo",
    #         "temperature": 0.8,
    #         "top_p": 1.0
    #     }

    #     headers = {
    #         "Content-Type": "text/plain",  # Invalid Content-Type
    #         "Authorization": "Bearer sk-the-mock-key-for-openai-simulation-on-localhost",
    #         "OpenAI-Beta": "assistants=v2"
    #     }

    #     response = client.post("/v1/assistants", json=request_data, headers=headers)
    #     self.assertEqual(response.status_code, 415)
    #     self.assertDictEqual(response.json(), {"detail": "Unsupported Media Type"})

    def test_create_assistant_with_invalid_openai_beta(self):
        request_data = {
            "instructions": "You are a personal math tutor.",
            "name": "Math Tutor",
            "description": "The description of the assistant.",
            "tools": [{"type": "code_interpreter"}],
            "model": "gpt-4-turbo",
            "temperature": 0.8,
            "top_p": 1.0
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer sk-the-mock-key-for-openai-simulation-on-localhost",
            "OpenAI-Beta": "invalid_value"  # Invalid OpenAI-Beta header
        }

        response = client.post("/v1/assistants", json=request_data, headers=headers)
        self.assertEqual(response.status_code, 400)
        self.assertDictEqual(response.json(), {"detail": "Invalid OpenAI-Beta header"})

unittest.main(exit=False)
#+end_src

#+RESULTS:
: ...
: ----------------------------------------------------------------------
: Ran 3 tests in 0.015s
: 
: OK


### Explanation of Test Cases

- Each test case verifies the behavior of the `create_assistant`
  function with respect to different header values.
- It includes tests for valid headers, invalid authorization tokens,
  incorrect content types, and invalid OpenAI-Beta values.

This updated implementation should resolve the issues you faced with
validating request headers, and it will pass the unit tests as
expected. You can run your application and tests in Emacs Org-mode as
you normally do.

