#+BEGIN_EXPORT html
---
layout: post
title: How to harness
tagline: " all your cores"
permalink: /python/processes.html
categories: [Python]
tags: [concurrent computation]
---
#+END_EXPORT

#+STARTUP: showall
#+OPTIONS: tags:nil num:nil \n:nil @:t ::t |:t ^:{} _:{} *:t
#+TOC: headlines 2
#+PROPERTY:header-args :results output :exports both :eval no-export

* The Main Purpose

  Is to unvail all subtleties about how-to utilize the power of all
  CPU's cores and avert its overloading simultaneously.

* Incentives

  #+BEGIN_QUOTE
  A context switch may happen on a core every fixed amount of time (a
  time slice) because the CPU automatically runs some kernel code
  periodically to permit preemption. Depending on the scheduler's
  rules, a task can be run for many time slices. A context switch can
  also occur when a thread calls functions that makes it unrunnable
  (eg. waiting for IO).
  #+END_QUOTE

  I.e. =Linux= kernel spreads load automatically across all cores
  /sequentially/ and *not* uniformly. E.g. if some particular process
  demands a lot of computation it should be /sequentially executed/ at
  all CPU's cores one by one.

  Of course it's absolutely normal for habitual usage, but what if
  we'll take an attempt to spread some process across all CPU's cores
  /simultaneously/?

* Average approach

  Exactly to demonstrate /threads/ and /processeses/ behaviour on the
  multi-core CPU lets sequentially create:

  - one infinite computational /process/;
  - one finite computational /process/;
  - split the latter onto several /threads/;
  - spread the latter onto several cores.


  As result we should load /all/ cores with some particular
  computation and measure its duration.

* Single infinite computational process
