#+BEGIN_EXPORT html
---
layout: post
title: The time complexity
tagline: " of an algorithm"
permalink: /algorithms/time-complexity.html
categories: [algorithms, literate programming]
tags: [computation, complexity, analysis]
---
#+END_EXPORT

#+STARTUP: showall
#+OPTIONS: tags:nil num:nil \n:nil @:t ::t |:t ^:{} _:{} *:t
#+TOC: headlines 2

* Definition
  #+CAPTION: Graphs of number of operations, N vs input size, n for common complexities, assuming a coefficient of 1
  #+ATTR_HTML: :alt Time Complexity :title Time Complexity proportion :align right
  https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Comparison_computational_complexity.svg/250px-Comparison_computational_complexity.svg.png
  #+BEGIN_QUOTE
  In computer science, the /time complexity of an algorithm/ quantifies the
  amount of time taken by an algorithm to run as a function of the length of
  the string representing the input. The time complexity of an algorithm is
  commonly expressed using *big O* notation, which excludes coefficients and
  lower order terms. When expressed this way, the time complexity is said to
  be described asymptotically, i.e., as the input size goes to infinity. For
  example, if the time required by an algorithm on all inputs of size n is
  at most 5n^{3} + 3n for any n (bigger than some n_{0}), the asymptotic time
  complexity is O(n^{3}).

  The expression *O* is also called =Landau's symbol=.

  Time complexity is commonly estimated by counting the number of elementary
  operations performed by the algorithm, where an elementary operation takes
  a fixed amount of time to perform. Thus, the amount of time taken and the
  number of elementary operations performed by the algorithm differ by at
  most a constant factor.
  #+END_QUOTE

  In other words:
  #+BEGIN_QUOTE
  The number of (machine) instructions which a program executes during its
  running time is called its time complexity in computer science.
  #+END_QUOTE

* Samples
  
  | Name         | Running time | Example                |
  |--------------+--------------+------------------------|
  | constant     | O(1)         | statement              |
  | linear       | O(n)         | single_loop(statement) |
  | quadratic    | O(n^{2})     | double_loop(statement) |
  | logarithmic  | O(log_{2}n)  | binary search \alpha   |
  | linearithmic | n*O(log_{2}n) | \beta                  |


  \alpha The running time of the algorithm is proportional to the number of
  times N can be divided by 2. This is because the algorithm divides the
  working area in half with each iteration.
  
  \beta The running time consists of N loops (iterative or recursive)
  that are logarithmic, thus the algorithm is a combination of linear and
  logarithmic.

  The full [[https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities][list of common time complexities]]  

* Timing technique
  #+BEGIN_SRC python :eval noexport :exports both :results output
    import timeit

    def merge_dicts(*dict_args):
        """
        Given any number of dicts, shallow copy and merge into a new dict,
        precedence goes to key value pairs in latter dicts.
        """
        result = {}
        for dictionary in dict_args:
            result.update(dictionary)
        return result

    x = {'a': 1, 'b': 2}
    y = {'b': 3, 'c': 4}
    print(min(timeit.repeat(lambda: merge_dicts(x, y), number=100)))
    print(min(timeit.repeat(lambda: merge_dicts(x, y))))

    # min(timeit.repeat(lambda: {**x, **y})) ==> for Python 3.5

    # dictionary item overwrite caveat illustration
    z = {'c': 1}
    assert(merge_dicts(x, y, z)['c'] == 1)
  #+END_SRC

  #+RESULTS:
  : 0.0002650470000844507
  : 2.314010903000053
